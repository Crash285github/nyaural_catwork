{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BC39M1S_2r-"
      },
      "source": [
        "### <font color='#FF93D5'> [0] <u>Initialization</u> </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OX2d-69T-A-U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# remove residues from notebook\n",
        "# !rm -rf sample_data\n",
        "\n",
        "# get the zip containing the cat images\n",
        "#!wget https://github.com/Crash285github/nyaural_catwork/raw/main/data/cats.zip -P data/\n",
        "\n",
        "# extract the cats into data/\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('data/cats.zip', 'r') as cats:\n",
        "  cats.extractall('data')\n",
        "\n",
        "# remove the downloaded zip\n",
        "# !rm data/cats.zip\n",
        "os.remove('data/cats.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUwS5LNjOGjm"
      },
      "source": [
        "### <font color='#FF7580'>[1] <u>Data manipulation</u> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwCXVVGWOGjn"
      },
      "source": [
        "#### <font color='#777'> [1.0] Imports, parameters</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLDeQoJTOGjo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms as tforms\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "%matplotlib inline\n",
        "\n",
        "# how to divide the dataset later on\n",
        "train_frac, val_frac, test_frac = 7, 2, 1\n",
        "\n",
        "# run the training on cuda gpu if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88z-y2cOGjo"
      },
      "source": [
        "#### <font color='#C45A63'> [1.1] Dataset class</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ6IUTVQOGjp"
      },
      "outputs": [],
      "source": [
        "class CatDataset(Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        self.total_imgs = []\n",
        "\n",
        "        for img in os.listdir(main_dir):\n",
        "            if not img.endswith(\".jpg\"):\n",
        "                continue\n",
        "            img_loc = os.path.join(self.main_dir, img)\n",
        "            image = Image.open(img_loc).convert('RGB')\n",
        "            tensor_image = self.transform(image)\n",
        "            self.total_imgs.append(tensor_image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.total_imgs[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6qhjLLOGjp"
      },
      "source": [
        "#### <font color='#C45A63'>[1.2] Dataset details</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuUkPZODOGjq"
      },
      "outputs": [],
      "source": [
        "# image transformations\n",
        "transforms = tforms.Compose([\n",
        "  tforms.ToTensor(),\n",
        "  tforms.Grayscale()\n",
        "])\n",
        "\n",
        "# normalize divisions\n",
        "n_train_frac = train_frac / (train_frac + val_frac + test_frac)\n",
        "n_val_frac = val_frac / (train_frac + val_frac + test_frac)\n",
        "n_test_frac = test_frac / (train_frac + val_frac + test_frac)\n",
        "\n",
        "print(f'{(n_train_frac*100):.1f} % Training data')\n",
        "print(f'{(n_val_frac*100):.1f} % Validation data')\n",
        "print(f'{(n_test_frac*100):.1f} % Testing data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5aoGnVHOGjr"
      },
      "source": [
        "#### <font color='#C45A63'>[1.3] Create Datasets</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTKWz8dbOGjr"
      },
      "outputs": [],
      "source": [
        "# create the dataset\n",
        "dataset = CatDataset('data', transforms)\n",
        "\n",
        "# split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [n_train_frac, n_val_frac, n_test_frac])\n",
        "\n",
        "print(f'{len(train_dataset)}\\ttraining images')\n",
        "print(f'{len(val_dataset)}\\tvalidating images')\n",
        "print(f'{len(test_dataset)}\\ttesting images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKvz5NwOGjr"
      },
      "source": [
        "##### <font color='#777'>[1.3.1] Visualize Datasets </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7zVoAOeOGjs"
      },
      "outputs": [],
      "source": [
        "# how many images to show\n",
        "number_of_sample_rows = 4\n",
        "number_of_sample_cols = 12\n",
        "\n",
        "fig, axs = plt.subplots(number_of_sample_rows, number_of_sample_cols, figsize=(number_of_sample_cols*2+1,number_of_sample_rows*2+2))\n",
        "fig.suptitle(\"Example images\", fontsize=32)\n",
        "\n",
        "selected = np.random.choice(len(dataset), number_of_sample_cols * number_of_sample_rows, False)\n",
        "for ind, ax in zip(selected, axs.flatten()):\n",
        "    # we permute: (?, 64, 64) --> (64, 64, ?)\n",
        "    ax.imshow(dataset[ind].permute(1,2,0), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g-TpZjjOGjs"
      },
      "source": [
        "#### <font color='#C45A63'>[1.4] Create DataLoaders </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2ncA54kOGjs"
      },
      "outputs": [],
      "source": [
        "batch_size=24\n",
        "\n",
        "# training loader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# validation loader\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# testing loader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9m5eTwJOGjt"
      },
      "source": [
        "### <font color='#FFEC72'>[2] <u>Define Classes</u> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2IoQzaeOGjt"
      },
      "source": [
        "#### <font color='#CEBD5C'> [2.1] Dense AutoEncoder</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8fxJMUGOGju"
      },
      "outputs": [],
      "source": [
        "class DenseAutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DenseAutoEncoder, self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "      nn.Flatten(), # (N, 64, 64) --> (N, 4096)\n",
        "      nn.Linear(64**2, 48**2), # (N, 4096) --> (N, 256)\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(48**2, 32**2),\n",
        "    )\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "      nn.Linear(32**2, 64**2),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "\n",
        "    return decoded.reshape(-1,1,64,64)\n",
        "\n",
        "  def regularization(self):\n",
        "    return 0 # weight_decay is in optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihpPZ-PDOGju"
      },
      "source": [
        "#### <font color='#CEBD5C'> [2.2] Sparse AutoEncoder</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTmdRxK9OGju"
      },
      "outputs": [],
      "source": [
        "class SparseAutoEncoder(nn.Module):\n",
        "  def __init__(self, l1_lam=0.001):\n",
        "    super(SparseAutoEncoder, self).__init__()\n",
        "    self.l1_lam  = l1_lam\n",
        "    self.encoder = nn.Sequential(\n",
        "      nn.Flatten(), # (N, 64, 64) --> (N, 4096)\n",
        "      nn.Linear(64**2, 48**2), # (N, 4096) --> (N, 256)\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(48**2, 32**2),\n",
        "    )\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "      nn.Linear(32**2, 48**2),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(48**2, 64**2),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "\n",
        "    return decoded.reshape(-1,1,64,64)\n",
        "\n",
        "  def regularization(self):\n",
        "    l1_norm = sum(torch.linalg.norm(p, 1) for p in self.parameters())\n",
        "    return self.l1_lam * l1_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgZk3VTb5tik"
      },
      "source": [
        "#### <font color='#CEBD5C'> [2.3] EarlyStopper</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfkVu0NT5_9J"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, min_delta=0., model: nn.Module | None = None):\n",
        "        self.patience: int = patience\n",
        "        self.min_delta: float = min_delta\n",
        "        self.counter: int = 0\n",
        "        self.min_validation_loss: float = np.inf\n",
        "        self.__model: nn.Module | None = model\n",
        "        self.__state_dict = None\n",
        "\n",
        "    def __call__(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "            if self.__model is not None:\n",
        "                self.__state_dict = deepcopy(self.__model.state_dict())\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        if self.__model is not None and self.__state_dict is not None:\n",
        "            with torch.no_grad():\n",
        "                self.__model.load_state_dict(self.__state_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5FgIisqOGjv"
      },
      "source": [
        "### <font color='#77FF5B'>[3] <u>Training Loop</u> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKSnq8tC9ga"
      },
      "source": [
        "#### <font color='#5EAA4C'> [3.1] Training, evaluation & testing functions</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40lWO_0Y8V9u"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, criterion):\n",
        "  model.train()\n",
        "  losses = []\n",
        "  for i, image in enumerate(data_loader):\n",
        "    print(f'\\r Training:\\t{(i*100 / len(data_loader)):.1f}%', end='')\n",
        "    image = image.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    reconstructed_image = model(image)\n",
        "    loss = criterion(reconstructed_image, image) + model.regularization()\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "  print('\\r Training:\\t100.0%', end='')\n",
        "  return np.mean(losses)\n",
        "\n",
        "def test_model(model, data_loader, criterion, phase):\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  with torch.no_grad():\n",
        "    for i, image in enumerate(data_loader):\n",
        "      print(f'\\r {phase}:\\t{(i*100 / len(data_loader)):.1f}%', end='')\n",
        "      image = image.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      reconstructed_image = model(image)\n",
        "      loss = criterion(reconstructed_image, image) + model.regularization()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "    print(f'\\r {phase}:\\t100.0%', end='')\n",
        "    return np.mean(losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI2r_6yJOGjw"
      },
      "source": [
        "#### <font color='#5EAA4C'> [3.2] Train DenseAutoEncoder</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cjL8TovOGjw"
      },
      "outputs": [],
      "source": [
        "# Dense model\n",
        "dense_model = DenseAutoEncoder().to(device)\n",
        "dense_criterion = nn.MSELoss()\n",
        "dense_optimizer = optim.Adam(dense_model.parameters(), lr=0.0004, weight_decay=0.00001)\n",
        "\n",
        "early_stopper = EarlyStopper(model=dense_model, patience=10)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(1000):\n",
        "  print(f'Epoch {epoch+1}:')\n",
        "\n",
        "  train_loss = train_one_epoch(dense_model, dense_optimizer, train_dataloader, dense_criterion)\n",
        "  print(f' | loss: {train_loss:.6f}')\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  val_loss = test_model(dense_model, val_dataloader, dense_criterion, phase='Validating')\n",
        "  print(f' | loss: {val_loss:.6f}')\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  test_loss = test_model(dense_model, test_dataloader, dense_criterion, phase='Testing')\n",
        "  print(f' | loss: {test_loss:.6f}')\n",
        "  test_losses.append(test_loss)\n",
        "\n",
        "  print('=' * 20)\n",
        "\n",
        "  if early_stopper(val_loss):\n",
        "    print(\"Early stopped\")\n",
        "    early_stopper.load_checkpoint()\n",
        "    break\n",
        "\n",
        "x = np.arange(epoch+1) + 1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x,train_losses, label='training loss', color='red')\n",
        "ax.plot(x,val_losses, label='validation loss', linestyle='--', color='orange')\n",
        "ax.plot(x,test_losses, label='test loss', linestyle=':', color='green')\n",
        "ax.set(xlabel='Epochs')\n",
        "\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxPW9BUaZqzC"
      },
      "source": [
        "#### <font color='#5EAA4C'> [3.3] Train SparseAutoEncoder</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKPiuALfZx4Q"
      },
      "outputs": [],
      "source": [
        "# Sparse model\n",
        "sparse_model = SparseAutoEncoder(l1_lam=0.0006).to(device)\n",
        "sparse_criterion = nn.MSELoss()\n",
        "sparse_optimizer = optim.Adam(sparse_model.parameters(), lr=0.00014)\n",
        "\n",
        "early_stopper = EarlyStopper(model=sparse_model, patience=5)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(1000):\n",
        "  print(f'Epoch {epoch+1}:')\n",
        "\n",
        "  train_loss = train_one_epoch(sparse_model, sparse_optimizer, train_dataloader, sparse_criterion)\n",
        "  l1_to_loss_percentage = (sparse_model.regularization() / train_loss *100)\n",
        "  print(f' | loss: {train_loss:.7f}, l1: {sparse_model.regularization():.7f} ({l1_to_loss_percentage:.0f}%)')\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  prev_val_loss = val_loss\n",
        "  val_loss = test_model(sparse_model, val_dataloader, sparse_criterion, phase='Validating')\n",
        "  print(f' | loss: {val_loss:.7f}')\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  test_loss = test_model(sparse_model, test_dataloader, sparse_criterion, phase='Testing')\n",
        "  print(f' | loss: {test_loss:.7f}')\n",
        "  test_losses.append(test_loss)\n",
        "\n",
        "  print('=' * 20)\n",
        "\n",
        "  if early_stopper(val_loss):\n",
        "    print(\"Early stopped\")\n",
        "    early_stopper.load_checkpoint()\n",
        "    break\n",
        "\n",
        "x = np.arange(epoch+1) + 1\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x,train_losses, label='training loss', color='red')\n",
        "ax.plot(x,val_losses, label='validation loss', linestyle='--', color='orange')\n",
        "ax.plot(x,test_losses, label='test loss', linestyle=':', color='green')\n",
        "ax.set(xlabel='Epochs')\n",
        "\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXcXtUvrVJVo"
      },
      "source": [
        "#### <font color='#5EAA4C'> [3.3] Visualize original and reconstructed images</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdj6cMnjfIA-"
      },
      "outputs": [],
      "source": [
        "number_of_samples = 7\n",
        "\n",
        "dense_model.eval()\n",
        "sparse_model.eval()\n",
        "\n",
        "selected = np.random.choice(len(test_dataset), number_of_samples, False)\n",
        "selected_tests = [test_dataset[i] for i in selected]\n",
        "\n",
        "# Original images\n",
        "fig, axs = plt.subplots(1, number_of_samples)\n",
        "fig.set_figwidth(24)\n",
        "fig.suptitle(\"Original images\", fontsize=32)\n",
        "for image, ax in zip(selected_tests, axs.flatten()):\n",
        "    ax.imshow(image.permute(1,2,0), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Dense model reconstructed images\n",
        "  fig, axs = plt.subplots(1, number_of_samples)\n",
        "  fig.set_figwidth(24)\n",
        "  fig.suptitle(\"Dense reconstructed images\", fontsize=32)\n",
        "  for image, ax in zip(selected_tests, axs):\n",
        "      reconstructed_image = dense_model(image.to(device)).to('cpu').detach().reshape(-1,64,64)\n",
        "      ax.imshow(reconstructed_image.permute(1,2,0), cmap='gray')\n",
        "      ax.axis('off')\n",
        "\n",
        "  # Sparse model reconstructed images\n",
        "  fig, axs = plt.subplots(1, number_of_samples)\n",
        "  fig.set_figwidth(24)\n",
        "  fig.suptitle(\"Sparse reconstructed images\", fontsize=32)\n",
        "  for image, ax in zip(selected_tests, axs):\n",
        "      reconstructed_image = sparse_model(image.to(device)).to('cpu').detach().reshape(-1,64,64)\n",
        "      ax.imshow(reconstructed_image.permute(1,2,0), cmap='gray')\n",
        "      ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Byd_DqUO_v"
      },
      "source": [
        "#### <font color='#5EAA4C'> [3.4] Visualize Sparse reprezentation</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTEs3hFTUX-j"
      },
      "outputs": [],
      "source": [
        "idxs = np.random.choice(np.arange(32**2), 100, False)\n",
        "fig, axs = plt.subplots(10,10, figsize=(30,30))\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      onehot = torch.zeros(32**2)\n",
        "      onehot[idxs[10*i + j]] = 1\n",
        "      out = sparse_model.decoder(onehot.to(device)).detach().cpu().numpy()\n",
        "      out = out.reshape(1,64,64).transpose([1,2,0])\n",
        "      axs[i,j].imshow(out, cmap='gray')\n",
        "      axs[i,j].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9AYxtWgia_5"
      },
      "outputs": [],
      "source": [
        "# done\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "except:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
